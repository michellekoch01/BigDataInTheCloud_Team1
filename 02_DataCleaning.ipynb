{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "23f6cf4a-0423-4081-aa50-68c5f4e4c5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: [FileInfo(path=&#39;dbfs:/mnt/input/ch.csv_already_loaded_20230923173513&#39;, name=&#39;ch.csv_already_loaded_20230923173513&#39;, size=2887293),\n",
       " FileInfo(path=&#39;dbfs:/mnt/input/es.csv&#39;, name=&#39;es.csv&#39;, size=2934135),\n",
       " FileInfo(path=&#39;dbfs:/mnt/input/gb.csv_already_loaded_20230924061856&#39;, name=&#39;gb.csv_already_loaded_20230924061856&#39;, size=5868673),\n",
       " FileInfo(path=&#39;dbfs:/mnt/input/se.csv_already_loaded_20230923172512&#39;, name=&#39;se.csv_already_loaded_20230923172512&#39;, size=2931936)]</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: [FileInfo(path=&#39;dbfs:/mnt/input/ch.csv_already_loaded_20230923173513&#39;, name=&#39;ch.csv_already_loaded_20230923173513&#39;, size=2887293),\n FileInfo(path=&#39;dbfs:/mnt/input/es.csv&#39;, name=&#39;es.csv&#39;, size=2934135),\n FileInfo(path=&#39;dbfs:/mnt/input/gb.csv_already_loaded_20230924061856&#39;, name=&#39;gb.csv_already_loaded_20230924061856&#39;, size=5868673),\n FileInfo(path=&#39;dbfs:/mnt/input/se.csv_already_loaded_20230923172512&#39;, name=&#39;se.csv_already_loaded_20230923172512&#39;, size=2931936)]</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Check files\n",
    "dbutils.fs.ls('/mnt/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c759ebf8-532f-4ee8-b873-afb7ff1959da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+--------------------+--------------------+-------+--------------------+----------+--------------+----------+--------------+------+---------+-------+\n",
       "               start|                 end|   load|        Last_Cleaned|Start_Date|    Start_Time|  End_Date|      End_Time|Season|Day_Night|Country|\n",
       "+--------------------+--------------------+-------+--------------------+----------+--------------+----------+--------------+------+---------+-------+\n",
       "2015-01-01 00:00:...|2015-01-01 01:00:...|15471.0|2023-09-24 09:07:...|2015-01-01|00:00:00+00:00|2015-01-01|01:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 01:00:...|2015-01-01 02:00:...|15105.0|2023-09-24 09:07:...|2015-01-01|01:00:00+00:00|2015-01-01|02:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 02:00:...|2015-01-01 03:00:...|14883.0|2023-09-24 09:07:...|2015-01-01|02:00:00+00:00|2015-01-01|03:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 03:00:...|2015-01-01 04:00:...|14755.0|2023-09-24 09:07:...|2015-01-01|03:00:00+00:00|2015-01-01|04:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 04:00:...|2015-01-01 05:00:...|14903.0|2023-09-24 09:07:...|2015-01-01|04:00:00+00:00|2015-01-01|05:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 05:00:...|2015-01-01 06:00:...|15131.0|2023-09-24 09:07:...|2015-01-01|05:00:00+00:00|2015-01-01|06:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 06:00:...|2015-01-01 07:00:...|15415.0|2023-09-24 09:07:...|2015-01-01|06:00:00+00:00|2015-01-01|07:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 07:00:...|2015-01-01 08:00:...|15567.0|2023-09-24 09:07:...|2015-01-01|07:00:00+00:00|2015-01-01|08:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 08:00:...|2015-01-01 09:00:...|15852.0|2023-09-24 09:07:...|2015-01-01|08:00:00+00:00|2015-01-01|09:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 09:00:...|2015-01-01 10:00:...|16041.0|2023-09-24 09:07:...|2015-01-01|09:00:00+00:00|2015-01-01|10:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 10:00:...|2015-01-01 11:00:...|16263.0|2023-09-24 09:07:...|2015-01-01|10:00:00+00:00|2015-01-01|11:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 11:00:...|2015-01-01 12:00:...|16628.0|2023-09-24 09:07:...|2015-01-01|11:00:00+00:00|2015-01-01|12:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 12:00:...|2015-01-01 13:00:...|16827.0|2023-09-24 09:07:...|2015-01-01|12:00:00+00:00|2015-01-01|13:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 13:00:...|2015-01-01 14:00:...|16895.0|2023-09-24 09:07:...|2015-01-01|13:00:00+00:00|2015-01-01|14:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 14:00:...|2015-01-01 15:00:...|17204.0|2023-09-24 09:07:...|2015-01-01|14:00:00+00:00|2015-01-01|15:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 15:00:...|2015-01-01 16:00:...|17483.0|2023-09-24 09:07:...|2015-01-01|15:00:00+00:00|2015-01-01|16:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 16:00:...|2015-01-01 17:00:...|17222.0|2023-09-24 09:07:...|2015-01-01|16:00:00+00:00|2015-01-01|17:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 17:00:...|2015-01-01 18:00:...|17014.0|2023-09-24 09:07:...|2015-01-01|17:00:00+00:00|2015-01-01|18:00:00+00:00|Winter|      Day|     no|\n",
       "2015-01-01 18:00:...|2015-01-01 19:00:...|16827.0|2023-09-24 09:07:...|2015-01-01|18:00:00+00:00|2015-01-01|19:00:00+00:00|Winter|    Night|     no|\n",
       "2015-01-01 19:00:...|2015-01-01 20:00:...|16536.0|2023-09-24 09:07:...|2015-01-01|19:00:00+00:00|2015-01-01|20:00:00+00:00|Winter|    Night|     no|\n",
       "+--------------------+--------------------+-------+--------------------+----------+--------------+----------+--------------+------+---------+-------+\n",
       "only showing top 20 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+--------------------+--------------------+-------+--------------------+----------+--------------+----------+--------------+------+---------+-------+\n|               start|                 end|   load|        Last_Cleaned|Start_Date|    Start_Time|  End_Date|      End_Time|Season|Day_Night|Country|\n+--------------------+--------------------+-------+--------------------+----------+--------------+----------+--------------+------+---------+-------+\n|2015-01-01 00:00:...|2015-01-01 01:00:...|15471.0|2023-09-24 09:07:...|2015-01-01|00:00:00+00:00|2015-01-01|01:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 01:00:...|2015-01-01 02:00:...|15105.0|2023-09-24 09:07:...|2015-01-01|01:00:00+00:00|2015-01-01|02:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 02:00:...|2015-01-01 03:00:...|14883.0|2023-09-24 09:07:...|2015-01-01|02:00:00+00:00|2015-01-01|03:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 03:00:...|2015-01-01 04:00:...|14755.0|2023-09-24 09:07:...|2015-01-01|03:00:00+00:00|2015-01-01|04:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 04:00:...|2015-01-01 05:00:...|14903.0|2023-09-24 09:07:...|2015-01-01|04:00:00+00:00|2015-01-01|05:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 05:00:...|2015-01-01 06:00:...|15131.0|2023-09-24 09:07:...|2015-01-01|05:00:00+00:00|2015-01-01|06:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 06:00:...|2015-01-01 07:00:...|15415.0|2023-09-24 09:07:...|2015-01-01|06:00:00+00:00|2015-01-01|07:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 07:00:...|2015-01-01 08:00:...|15567.0|2023-09-24 09:07:...|2015-01-01|07:00:00+00:00|2015-01-01|08:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 08:00:...|2015-01-01 09:00:...|15852.0|2023-09-24 09:07:...|2015-01-01|08:00:00+00:00|2015-01-01|09:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 09:00:...|2015-01-01 10:00:...|16041.0|2023-09-24 09:07:...|2015-01-01|09:00:00+00:00|2015-01-01|10:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 10:00:...|2015-01-01 11:00:...|16263.0|2023-09-24 09:07:...|2015-01-01|10:00:00+00:00|2015-01-01|11:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 11:00:...|2015-01-01 12:00:...|16628.0|2023-09-24 09:07:...|2015-01-01|11:00:00+00:00|2015-01-01|12:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 12:00:...|2015-01-01 13:00:...|16827.0|2023-09-24 09:07:...|2015-01-01|12:00:00+00:00|2015-01-01|13:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 13:00:...|2015-01-01 14:00:...|16895.0|2023-09-24 09:07:...|2015-01-01|13:00:00+00:00|2015-01-01|14:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 14:00:...|2015-01-01 15:00:...|17204.0|2023-09-24 09:07:...|2015-01-01|14:00:00+00:00|2015-01-01|15:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 15:00:...|2015-01-01 16:00:...|17483.0|2023-09-24 09:07:...|2015-01-01|15:00:00+00:00|2015-01-01|16:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 16:00:...|2015-01-01 17:00:...|17222.0|2023-09-24 09:07:...|2015-01-01|16:00:00+00:00|2015-01-01|17:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 17:00:...|2015-01-01 18:00:...|17014.0|2023-09-24 09:07:...|2015-01-01|17:00:00+00:00|2015-01-01|18:00:00+00:00|Winter|      Day|     no|\n|2015-01-01 18:00:...|2015-01-01 19:00:...|16827.0|2023-09-24 09:07:...|2015-01-01|18:00:00+00:00|2015-01-01|19:00:00+00:00|Winter|    Night|     no|\n|2015-01-01 19:00:...|2015-01-01 20:00:...|16536.0|2023-09-24 09:07:...|2015-01-01|19:00:00+00:00|2015-01-01|20:00:00+00:00|Winter|    Night|     no|\n+--------------------+--------------------+-------+--------------------+----------+--------------+----------+--------------+------+---------+-------+\nonly showing top 20 rows\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split, when, current_timestamp, lit\n",
    "from datetime import datetime\n",
    "\n",
    "# Delete all CSV files in outputdatabricks directory\n",
    "existing_directory_path = '/mnt/outputdatabricks/'\n",
    "csv_files = [file.name for file in dbutils.fs.ls(existing_directory_path) if file.name.endswith('.csv')]\n",
    "for csv_file in csv_files:\n",
    "    dbutils.fs.rm(existing_directory_path + csv_file)\n",
    "\n",
    "#Dictionary Country Data \n",
    "country_data = {\n",
    "    \"at\": {\"TotalPopulation\": 8958960, \"LandArea\": 82409.00},\n",
    "    \"be\": {\"TotalPopulation\": 11686140, \"LandArea\": 30280.00},\n",
    "    \"ch\": {\"TotalPopulation\": 8796669, \"LandArea\": 39516.00},\n",
    "    \"de\": {\"TotalPopulation\": 83294633, \"LandArea\": 348560.00},\n",
    "    \"dk\": {\"TotalPopulation\": 5910913, \"LandArea\": 42430.00},\n",
    "    \"es\": {\"TotalPopulation\": 47519628, \"LandArea\": 498800.00},\n",
    "    \"fr\": {\"TotalPopulation\": 64756584, \"LandArea\": 547557.00},\n",
    "    \"gb\": {\"TotalPopulation\": 67736802, \"LandArea\": 241930.00},\n",
    "    \"ie\": {\"TotalPopulation\": 5056935, \"LandArea\": 68890.00},\n",
    "    \"it\": {\"TotalPopulation\": 58870762, \"LandArea\": 294140.00},\n",
    "    \"lu\": {\"TotalPopulation\": 654768, \"LandArea\": 2590.00},\n",
    "    \"nl\": {\"TotalPopulation\": 17618299, \"LandArea\": 33720.00},\n",
    "    \"no\": {\"TotalPopulation\": 5474360, \"LandArea\": 365268.00},\n",
    "    \"pt\": {\"TotalPopulation\": 10247605, \"LandArea\": 91590.00},\n",
    "    \"se\": {\"TotalPopulation\": 10612086, \"LandArea\": 410340.00},\n",
    "    \"mk\": {\"TotalPopulation\": 00000000, \"LandArea\": 000000.00}\n",
    "}\n",
    "\n",
    "\n",
    "directory_path = '/mnt/input/'\n",
    "\n",
    "# 1. Reading the files\n",
    "file_list = [file.name for file in dbutils.fs.ls(directory_path) if \"already_loaded\" not in file.name]\n",
    "existing_directory_path = '/mnt/outputdatabricks/'\n",
    "\n",
    "for file in file_list:\n",
    "    # Read the file\n",
    "    df = spark.read.option(\"header\", \"true\").csv(directory_path + file)\n",
    "    \n",
    "    # 2. Cleaning the data\n",
    "    df = df.withColumn(\"Last_Cleaned\", current_timestamp())\n",
    "    \n",
    "    if \"start\" in df.columns and \"end\" in df.columns:\n",
    "        df = df.withColumn(\"Start_Date\", split(col(\"start\"), \" \")[0])\n",
    "        df = df.withColumn(\"Start_Time\", split(col(\"start\"), \" \")[1])\n",
    "        df = df.withColumn(\"End_Date\", split(col(\"end\"), \" \")[0])\n",
    "        df = df.withColumn(\"End_Time\", split(col(\"end\"), \" \")[1])\n",
    "        \n",
    "    df = df.withColumn(\"Season\", \n",
    "                      when(col(\"Start_Date\").substr(6, 2).cast(\"int\").isin([12, 1, 2]), \"Winter\")\n",
    "                      .when(col(\"Start_Date\").substr(6, 2).cast(\"int\").isin([3, 4, 5]), \"Spring\")\n",
    "                      .when(col(\"Start_Date\").substr(6, 2).cast(\"int\").isin([6, 7, 8]), \"Summer\")\n",
    "                      .otherwise(\"Autumn\"))\n",
    "    \n",
    "    df = df.withColumn(\"Day_Night\", \n",
    "                      when(col(\"Start_Time\").substr(1, 2).cast(\"int\").between(6, 17), \"Day\")\n",
    "                      .otherwise(\"Night\"))\n",
    "    \n",
    "    country_name = file[:2]\n",
    "    df = df.withColumn(\"Country\", lit(country_name))\n",
    "\n",
    "    # Add the new columns based on the country_data dictionary\n",
    "    df = df.withColumn(\"TotalPopulation\", lit(country_data[country_name][\"TotalPopulation\"]))\n",
    "    df = df.withColumn(\"LandArea\", lit(country_data[country_name][\"LandArea\"]))\n",
    "\n",
    "    df.show()\n",
    "    \n",
    "    # 3. Saving the cleaned data\n",
    "    # Construct the full path to the output file\n",
    "    temp_dir = existing_directory_path + \"temp/\"\n",
    "    df.write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(temp_dir)\n",
    "\n",
    "    current_utc_time = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "    output_file_path = existing_directory_path + f\"{country_name}_cleaned_{current_utc_time}.csv\"\n",
    "    \n",
    "    part_files = [f.name for f in dbutils.fs.ls(temp_dir) if 'part-' in f.name]\n",
    "    if part_files:\n",
    "        part_file_path = temp_dir + part_files[0]\n",
    "        dbutils.fs.mv(part_file_path, output_file_path)\n",
    "\n",
    "    # Clean up the temporary directory\n",
    "    dbutils.fs.rm(temp_dir, recurse=True)\n",
    "\n",
    "    # 4. Deleting the original files after processing\n",
    "    dbutils.fs.rm(directory_path + file)\n",
    "\n",
    "    # 4. Renaming the original files\n",
    "    #new_file_name = f\"{file}_already_loaded_{current_utc_time}\"\n",
    "    #dbutils.fs.mv(directory_path + file, directory_path + new_file_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_DataCleaning",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
